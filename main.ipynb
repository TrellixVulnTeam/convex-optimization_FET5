{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable, grad\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from data.stanford_dogs_data import dogs\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n_epoch = 2\n",
    "learning_rate = 0.01\n",
    "input_size=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_transforms = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(input_size, ratio=(1, 1.3)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "trainset = dogs(root='./data',train=True,cropped=False,transform=input_transforms,download=True)\n",
    "testset = dogs(root='./data',train=False,cropped=False,transform=input_transforms,download=True)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVRG Optimizer inherit from torch.optim.optimizer\n",
    "class SVRG(Optimizer):\n",
    "    def __init__(self, params, learn_rate=required, SVRG_inner_freq =20):\n",
    "        if learn_rate is not required and learn_rate < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(learn_rate))\n",
    "\n",
    "        defaults = dict(learn_rate=learn_rate, freq=SVRG_inner_freq)\n",
    "        self.counter = 0\n",
    "        self.counter2 = 0\n",
    "        self.flag = False\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super().__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            freq = group['freq']\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                w_grad = param.grad.data\n",
    "                param_state = self.state[param]\n",
    "                \n",
    "                if 'outer' not in param_state:\n",
    "                    u_hat = param_state['outer'] = torch.zeros_like(param.data)\n",
    "                    u_hat.add_(w_grad)\n",
    "                    w_hat = param_state['inner'] = torch.zeros_like(param.data)\n",
    "\n",
    "                u_hat = param_state['outer']\n",
    "                w_hat = param_state['inner']\n",
    "\n",
    "                if self.counter == freq:\n",
    "                    u_hat.data = w_grad.clone()\n",
    "                    temp = torch.zeros_like(param.data)\n",
    "                    w_hat.data = temp.clone()\n",
    "                    \n",
    "                if self.counter2 == 1:\n",
    "                    w_hat.data.add_(w_grad)\n",
    "\n",
    "                #dont update parameters when computing large batch (low variance gradients)\n",
    "                if self.counter != freq and self.flag != False:\n",
    "                    param.data.add_(-group['learn_rate'], (w_grad - w_hat + u_hat) )\n",
    "\n",
    "        self.flag = True\n",
    "        \n",
    "        if self.counter == freq:\n",
    "            self.counter = 0\n",
    "            self.counter2 = 0\n",
    "\n",
    "        self.counter += 1    \n",
    "        self.counter2 += 1\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(6)\n",
    "        self.dropout1 = nn.Dropout(p = 0.1)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
    "        self.dropout2 = nn.Dropout(p = 0.1)\n",
    "        self.fc1 = nn.Linear(16*53*53, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 120)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.pool(F.relu(self.batchnorm1(self.conv1(x)))))\n",
    "        x = self.dropout2(self.pool(F.relu(self.batchnorm2(self.conv2(x)))))\n",
    "        x = x.view(-1, 16*53*53)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def partial_grad(self, data, target, loss_function):\n",
    "        outputs = self.forward(data)\n",
    "        loss = loss_function(outputs, target)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    \n",
    "    def calculate_loss_grad(self, dataset, loss_function, n_samples):\n",
    "        total_loss = 0.0\n",
    "        full_grad = 0.0\n",
    "        for i_grad, data_grad in enumerate(dataset):\n",
    "            inputs, labels = data_grad\n",
    "            inputs, labels = Variable(inputs), Variable(labels) #wrap data and target into variable\n",
    "            total_loss += (1./n_samples) * self.partial_grad(inputs, labels, loss_function).item()\n",
    "        \n",
    "        for para in self.parameters():\n",
    "            full_grad += para.grad.data.norm(2)**2\n",
    "        \n",
    "        return total_loss, (1./n_samples) * np.sqrt(full_grad)\n",
    "    \n",
    "    def backward(self, dataset,testloader, loss_function, n_epoch, learning_rate):\n",
    "        total_loss_epoch = [0 for i in range(n_epoch)]\n",
    "        grad_norm_epoch = [0 for i in range(n_epoch)]\n",
    "        net.train()\n",
    "        record=[]\n",
    "        train_re=[]\n",
    "        test_re=[]\n",
    "        #SGD\n",
    "#         optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "        #SVRG2\n",
    "#         optimizer = SVRG(net.parameters(), lr=0.01)\n",
    "        #Adam\n",
    "#         optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "        iters=0\n",
    "        for epoch in range(n_epoch):\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            #SVRG1 parameters start\n",
    "            w_hat = copy.deepcopy(self)\n",
    "            u_hat = copy.deepcopy(self)\n",
    "            u_hat.zero_grad()\n",
    "            total_loss_epoch[epoch], grad_norm_epoch[epoch] = u_hat.calculate_loss_grad(dataset, loss_function, n_samples)\n",
    "            #SVRG1 parameters end\n",
    "            \n",
    "            for i_data, data in enumerate(dataset):\n",
    "                inputs, labels = data\n",
    "                #SGD,Adam,SVRG2\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = net(inputs)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "#                 running_loss += loss.item()\n",
    "\n",
    "                #SVRG1 backward start\n",
    "                w_hat.zero_grad() \n",
    "                prev_loss = w_hat.partial_grad(inputs, labels, loss_function)\n",
    "                self.zero_grad()\n",
    "                cur_loss = self.partial_grad(inputs, labels, loss_function)\n",
    "                for param1, param2, param3 in zip(self.parameters(), w_hat.parameters(), u_hat.parameters()): \n",
    "                    param1.data -= (learning_rate) * (param1.grad.data - param2.grad.data + (1./n_samples) * param3.grad.data)\n",
    "                running_loss += cur_loss.item()\n",
    "                #SVRG1 backward end\n",
    "                \n",
    "                iters+=1\n",
    "                \n",
    "                #evalue present weight accuracy\n",
    "                if iters%50==0:\n",
    "                    train_loss, _ = eval_net(dataset)\n",
    "                    test_loss, _ = eval_net(testloader)\n",
    "                    print(epoch)\n",
    "                    record.append(running_loss/50)\n",
    "                    train_re.append(train_loss)\n",
    "                    test_re.append(test_loss)\n",
    "                    running_loss=0.0\n",
    "                  \n",
    "        return total_loss_epoch, grad_norm_epoch,record,train_re,test_re\n",
    "def eval_net(dataloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    net.eval()\n",
    "    criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "    for data in dataloader:\n",
    "        images, labels = data\n",
    "        images, labels = Variable(images), Variable(labels)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels.data).sum()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "    net.train() \n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_samples = len(trainloader)\n",
    "print(n_samples)\n",
    "# SGD\n",
    "# record,Train_record,Test_record=[],[],[]\n",
    "#SVRG1\n",
    "record1,Train_record1,Test_record1=[],[],[]\n",
    "#Adam\n",
    "# record2,Train_record2,Test_record2=[],[],[]\n",
    "#SVRG2\n",
    "# record3,Train_record3,Test_record3=[],[],[]\n",
    "\n",
    "total_loss_epoch, grad_norm_epoch,record1,Train_record1,Test_record1= net.backward(trainloader,testloader, criterion, n_epoch, learning_rate)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(record1,label='Train')\n",
    "plt.plot(Train_record1,label='Train_valid')\n",
    "plt.plot(Test_record1,label='Test_valid')\n",
    "plt.title('SVRG1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(record,label='Train')\n",
    "plt.plot(Train_record,label='Train_valid')\n",
    "plt.plot(Test_record,label='Test_valid')\n",
    "plt.title('SGD')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
